import streamlit as st
import json
import numpy as np
from openai import OpenAI
import os
from dotenv import load_dotenv

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÏÏ…Î¸Î¼Î¯ÏƒÎµÏ‰Î½
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# 1. Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î£ÎµÎ»Î¯Î´Î±Ï‚
st.set_page_config(
    page_title="Vouli-AI: ÎÎ¿Î¼Î¹ÎºÏŒÏ‚ Î’Î¿Î·Î¸ÏŒÏ‚",
    page_icon="ğŸ›ï¸",
    layout="centered"
)


# 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
@st.cache_data
def load_knowledge_base():
    chunks = []
    if os.path.exists("chunks.jsonl"):
        with open("chunks.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                chunks.append(json.loads(line))
    return chunks


chunks = load_knowledge_base()

# --- UI Î•Î¦Î‘Î¡ÎœÎŸÎ“Î—Î£ ---
st.title("ğŸ›ï¸ï¸ Vouli-AI: ÎŸ Î¨Î·Ï†Î¹Î±ÎºÏŒÏ‚ ÏƒÎ¿Ï… Î’Î¿Î·Î¸ÏŒÏ‚ ÎÎ¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±Ï‚")
st.markdown("""
Î‘Ï…Ï„ÏŒÏ‚ Î¿ Î²Î¿Î·Î¸ÏŒÏ‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î¤ÎµÏ‡Î½Î·Ï„Î® ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î· Î³Î¹Î± Î½Î± Î±Î½Î±Î»ÏÎµÎ¹ Ï„Î¿ Î½Î¿Î¼Î¿Î¸ÎµÏ„Î¹ÎºÏŒ Î­ÏÎ³Î¿ Ï„Î·Ï‚ Î’Î¿Ï…Î»Î®Ï‚. 
ÎœÏ€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± ÏÏ‰Ï„Î®ÏƒÎµÏ„Îµ Î¿Ï„Î¹Î´Î®Ï€Î¿Ï„Îµ Î³Î¹Î± Ï„Î¿Ï…Ï‚ Î½ÏŒÎ¼Î¿Ï…Ï‚ Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ ÎºÎ±Ï„Î±Ï‡Ï‰ÏÎ·Î¸ÎµÎ¯ ÏƒÏ„Î¿ ÏƒÏÏƒÏ„Î·Î¼Î±.
""")

with st.sidebar:
    st.header("ğŸ“Œ Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚")
    st.write("ÎŸ Î²Î¿Î·Î¸ÏŒÏ‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ Ï„Î¿ API Ï„Î·Ï‚ Î’Î¿Ï…Î»Î®Ï‚.")
    if chunks:
        sources = list(set([c['source'] for c in chunks]))
        st.subheader("ğŸ“š Î•Î½ÎµÏÎ³Î¿Î¯ ÎÏŒÎ¼Î¿Î¹:")
        for s in sources:
            st.caption(f"â€¢ {s}")

st.divider()


# --- Î›ÎŸÎ“Î™ÎšÎ— Î‘Î Î‘ÎÎ¤Î—Î£Î•Î©Î ---
def get_answer(question):
    if not chunks:
        return "Î›Ï…Ï€Î¬Î¼Î±Î¹, Î±Î»Î»Î¬ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÏ„Î· Î²Î¬ÏƒÎ· Î¼Î¿Ï…."

    try:
        q_emb = client.embeddings.create(
            input=[question],
            model="text-embedding-ada-002"  # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ ÏƒÏ„Î±Î¸ÎµÏÏŒ
        ).data[0].embedding

        scores = []
        for ch in chunks:
            similarity = np.dot(q_emb, ch["embedding"])
            scores.append((similarity, ch))
        scores.sort(key=lambda x: x[0], reverse=True)

        context = "\n\n".join([f"[Î Î·Î³Î®: {ch['source']}, Î£ÎµÎ»: {ch['page']}]: {ch['text']}" for _, ch in scores[:5]])

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system",
                 "content": "Î•Î¯ÏƒÎ±Î¹ Î­Î¼Ï€ÎµÎ¹ÏÎ¿Ï‚ Î½Î¿Î¼Î¹ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚. Î‘Ï€Î¬Î½Ï„Î± Î²Î±ÏƒÎ¹ÏƒÎ¼Î­Î½Î¿Ï‚ Î‘Î ÎŸÎšÎ›Î•Î™Î£Î¤Î™ÎšÎ‘ ÏƒÏ„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿. Î‘Î½Î±Ï†Î­ÏÎµ Î Î·Î³Î® ÎºÎ±Î¹ Î£ÎµÎ»Î¯Î´Î±."},
                {"role": "user", "content": f"ÎšÎµÎ¯Î¼ÎµÎ½Î¿:\n{context}\n\nÎ•ÏÏÏ„Î·ÏƒÎ·: {question}"}
            ],
            temperature=0.2
        )
        return response.choices[0].message.content
    except Exception as e:
        return "âš ï¸ Î ÏÎ¿ÏƒÏ‰ÏÎ¹Î½ÏŒ ÏƒÏ†Î¬Î»Î¼Î± ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚. Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Î¾Î±Î½Î¬."


# --- CHAT INTERFACE ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Î ÏÏ‚ Î¼Ï€Î¿ÏÏ Î½Î± Î²Î¿Î·Î¸Î®ÏƒÏ‰;"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)
    with st.chat_message("assistant"):
        answer = get_answer(prompt)
        st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})