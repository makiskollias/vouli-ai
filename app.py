import streamlit as st
import json
import numpy as np
from openai import OpenAI
import os
from dotenv import load_dotenv

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÏÏ…Î¸Î¼Î¯ÏƒÎµÏ‰Î½
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# 1. Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î£ÎµÎ»Î¯Î´Î±Ï‚
st.set_page_config(
    page_title="Vouli-AI: Î’Î¿Î·Î¸ÏŒÏ‚ ÎÎ¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±Ï‚",
    page_icon="ğŸ›ï¸",
    layout="centered"
)

# 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
@st.cache_data
def load_knowledge_base():
    chunks = []
    if os.path.exists("chunks.jsonl"):
        with open("chunks.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                chunks.append(json.loads(line))
    return chunks

chunks = load_knowledge_base()

# --- UI Î•Î¦Î‘Î¡ÎœÎŸÎ“Î—Î£ ---
st.title("ğŸ›ï¸ Vouli-AI: ÎŸ Î¨Î·Ï†Î¹Î±ÎºÏŒÏ‚ ÏƒÎ¿Ï… Î’Î¿Î·Î¸ÏŒÏ‚ ÎÎ¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±Ï‚")
st.markdown("""
Î‘Ï…Ï„ÏŒÏ‚ Î¿ Î²Î¿Î·Î¸ÏŒÏ‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î¤ÎµÏ‡Î½Î·Ï„Î® ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î· Î³Î¹Î± Î½Î± Î±Ï€Î±Î½Ï„Î¬ **Î¼ÏŒÎ½Î¿** Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿Ï…Ï‚ Î½ÏŒÎ¼Î¿Ï…Ï‚ Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ ÎºÎ±Ï„Î±Ï‡Ï‰ÏÎ·Î¸ÎµÎ¯ ÏƒÏ„Î¿ ÏƒÏÏƒÏ„Î·Î¼Î±,
Ï€Î±ÏÎ±Î¸Î­Ï„Î¿Î½Ï„Î±Ï‚ **Î Î·Î³Î® & Î£ÎµÎ»Î¯Î´Î±**.
""")

with st.sidebar:
    st.header("ğŸ“Œ Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚")
    st.write("ÎŸ Î²Î¿Î·Î¸ÏŒÏ‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ Ï„Î¿ API Ï„Î·Ï‚ Î’Î¿Ï…Î»Î®Ï‚ / ÎµÏ€Î¯ÏƒÎ·Î¼Î± ÎºÎµÎ¯Î¼ÎµÎ½Î± Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ ÎµÎ¹ÏƒÎ±Ï‡Î¸ÎµÎ¯.")
    st.divider()

    top_k = st.slider("ğŸ” Î‘Ï€Î¿ÏƒÏ€Î¬ÏƒÎ¼Î±Ï„Î± (top-k)", 3, 12, 6, 1)
    min_sim = st.slider("ğŸ›¡ï¸ Î•Î»Î¬Ï‡Î¹ÏƒÏ„Î· Î¿Î¼Î¿Î¹ÏŒÏ„Î·Ï„Î± (threshold)", 0.10, 0.40, 0.18, 0.01)

    if chunks:
        sources = sorted(list(set([c.get('source', 'Î†Î³Î½Ï‰ÏƒÏ„Î· Î Î·Î³Î®') for c in chunks])))
        st.subheader("ğŸ“š Î•Î½ÎµÏÎ³Î¿Î¯ ÎÏŒÎ¼Î¿Î¹/Î Î·Î³Î­Ï‚:")
        for s in sources:
            st.caption(f"â€¢ {s}")

st.divider()

# --- Î’ÎŸÎ—Î˜Î—Î¤Î™ÎšÎ‘ ---
def cosine_sim(a, b) -> float:
    a = np.array(a, dtype=np.float32)
    b = np.array(b, dtype=np.float32)
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

@st.cache_data(show_spinner=False)
def embed_text(text: str):
    # Î ÏÎ¿Ï„ÎµÎ¹Î½ÏŒÎ¼ÎµÎ½Î¿ Ï€Î¹Î¿ ÏƒÏÎ³Ï‡ÏÎ¿Î½Î¿ embedding model:
    # text-embedding-3-small (value) Î® text-embedding-3-large (quality)
    return client.embeddings.create(
        input=[text],
        model="text-embedding-3-small"
    ).data[0].embedding

def build_context(scored_chunks):
    # Î£Ï…Î¼Î¼Î±Î¶ÎµÎ¼Î­Î½Î· Î¼Î¿ÏÏ†Î¿Ï€Î¿Î¯Î·ÏƒÎ· context Î¼Îµ ÏƒÎ±Ï†ÎµÎ¯Ï‚ Î±Î½Î±Ï†Î¿ÏÎ­Ï‚
    parts = []
    for sim, ch in scored_chunks:
        source = ch.get("source", ch.get("source_id", "Î†Î³Î½Ï‰ÏƒÏ„Î· Î Î·Î³Î®"))
        page = ch.get("page", "?")
        text = ch.get("text", "")
        parts.append(f"[Î Î·Î³Î®: {source} | Î£ÎµÎ»Î¯Î´Î±: {page}]\n{text}")
    return "\n\n---\n\n".join(parts)

# --- Î›ÎŸÎ“Î™ÎšÎ— Î‘Î Î‘ÎÎ¤Î—Î£Î•Î©Î ---
def get_answer(question: str) -> str:
    if not chunks:
        return "Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±ÎºÏŒÎ¼Î· Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÏ„Î· Î²Î¬ÏƒÎ·. Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ ÎµÎ¹ÏƒÎ±Î³Ï‰Î³Î® Î½ÏŒÎ¼Ï‰Î½/ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½."

    try:
        q_emb = embed_text(question)

        scores = []
        for ch in chunks:
            ch_emb = ch.get("embedding")
            if not ch_emb:
                continue
            sim = cosine_sim(q_emb, ch_emb)
            scores.append((sim, ch))

        if not scores:
            return "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ embeddings ÏƒÏ„Î· Î²Î¬ÏƒÎ· (Î»ÎµÎ¯Ï€ÎµÎ¹ Ï„Î¿ Ï€ÎµÎ´Î¯Î¿ 'embedding' ÏƒÏ„Î± chunks)."

        scores.sort(key=lambda x: x[0], reverse=True)
        top = scores[:top_k]

        # Guardrail: Î±Î½ Î±ÎºÏŒÎ¼Î± ÎºÎ±Î¹ Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± ÎµÎ¯Î½Î±Î¹ Ï‡Î±Î¼Î·Î»ÏŒ, ÎºÎ±Î»ÏÏ„ÎµÏÎ± â€œÎ´ÎµÎ½ Î¾Î­ÏÏ‰â€
        if top[0][0] < min_sim:
            return (
                "Î”ÎµÎ½ Î²ÏÎ®ÎºÎ± ÎµÏ€Î±ÏÎºÎ­Ï‚ ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒ Î±Ï€ÏŒÏƒÏ€Î±ÏƒÎ¼Î± ÏƒÏ„Î± Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î± ÎºÎµÎ¯Î¼ÎµÎ½Î± Î³Î¹Î± Î½Î± Î±Ï€Î±Î½Ï„Î®ÏƒÏ‰ Î¼Îµ Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±.\n\n"
                "Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î½Î±:\n"
                "- Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒ Î½ÏŒÎ¼Î¿Ï…/Î¬ÏÎ¸ÏÎ¿/Ï€Î±ÏÎ¬Î³ÏÎ±Ï†Î¿, Î®\n"
                "- ÎºÎ¬Î½ÎµÎ¹Ï‚ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ· Ï€Î¹Î¿ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î·."
            )

        context = build_context(top)

        system_prompt = (
            "Î•Î¯ÏƒÎ±Î¹ Î²Î¿Î·Î¸ÏŒÏ‚ Î½Î¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±Ï‚.\n"
            "ÎšÎ±Î½ÏŒÎ½ÎµÏ‚:\n"
            "1) Î‘Ï€Î¬Î½Ï„Î± ÎœÎŸÎÎŸ Î±Ï€ÏŒ Ï„Î¿ Î´Î¿ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿.\n"
            "2) Î“Î¹Î± ÎšÎ‘Î˜Î• Î¹ÏƒÏ‡Ï…ÏÎ¹ÏƒÎ¼ÏŒ/Ï€ÏÏŒÏ„Î±ÏƒÎ· Ï€Î¿Ï… Î±Ï†Î¿ÏÎ¬ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Î½ÏŒÎ¼Î¿Ï…, Î²Î¬Î»Îµ Ï€Î±ÏÎ±Ï€Î¿Î¼Ï€Î® Î¼Îµ Î¼Î¿ÏÏ†Î®: (Î Î·Î³Î®, Î£ÎµÎ»Î¯Î´Î±).\n"
            "3) Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÎ±Ï†Î®Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î± ÏƒÏ„Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿, Ï€ÎµÏ‚ Î¾ÎµÎºÎ¬Î¸Î±ÏÎ±: Â«Î”ÎµÎ½ Ï€ÏÎ¿ÎºÏÏ€Ï„ÎµÎ¹ Î±Ï€ÏŒ Ï„Î± Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î± Î±Ï€Î¿ÏƒÏ€Î¬ÏƒÎ¼Î±Ï„Î±Â».\n"
            "4) ÎœÎ·Î½ Ï€ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹Ï‚ ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÎ® Î³Î½ÏÏƒÎ· ÎºÎ±Î¹ Î¼Î·Î½ ÎºÎ¬Î½ÎµÎ¹Ï‚ ÎµÎ¹ÎºÎ±ÏƒÎ¯ÎµÏ‚.\n"
        )

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ÎšÎµÎ¯Î¼ÎµÎ½Î¿:\n{context}\n\nÎ•ÏÏÏ„Î·ÏƒÎ·: {question}"}
            ],
            temperature=0.2
        )
        return response.choices[0].message.content

    except Exception:
        return "âš ï¸ Î ÏÎ¿ÏƒÏ‰ÏÎ¹Î½ÏŒ ÏƒÏ†Î¬Î»Î¼Î±. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î¾Î±Î½Î¬."

# --- CHAT INTERFACE ---

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Î ÏÏ‚ Î¼Ï€Î¿ÏÏ Î½Î± Î²Î¿Î·Î¸Î®ÏƒÏ‰;"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Î£ÎºÎ­Ï†Ï„ÎµÏ„Î±Î¹..."):
            answer = get_answer(prompt)
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})